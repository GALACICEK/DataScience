{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Building a decision tree with the training set\n",
    "\n",
    "Classes are: Drug Drug\n",
    "\n",
    "Decision Tree:\n",
    "\n",
    "- age\n",
    "    - young\n",
    "        - sex\n",
    "            - F\n",
    "            -M\n",
    "\n",
    "    - middle-age\n",
    "\n",
    "    - senior\n",
    "        - cholesterol\n",
    "            - high\n",
    "            - normal\n",
    "\n",
    "    Each \"internal node\" corresponds to a test.\n",
    "    And each \"branch\" corresponds to a result of the test.\n",
    "    And each \"leaf node\" assigns a patient to a class.\n",
    "\n",
    "1- Choose an attribute from dataset 2- Calculate the significance of the attribute in the splitting of the data. 3- Split the data based on the value of the best attribute. 4- Go to step 1.\n",
    "#### Building Decision Trees\n",
    "\n",
    "Decision trees are built using recursive partitioning to classify the data.\n",
    "\n",
    "- More predictiveness\n",
    "- Less Impurity\n",
    "- Lower Entropy\n",
    "\n",
    "\"Pure Node\": A node in the tree is considered pure if, in all of the cases, the nodes fall into a spesific category.\n",
    "Entropy\n",
    "\n",
    "Information disorder or randomness in the data. The entropy is used to calculate the homogeneity of the samples in that node.\n",
    "\n",
    "- TODO Check entropy formula\n",
    "\n",
    "\"The tree with the higher information gain after splitting.\" Information gain = Entropy before split - Weighted entropy after split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
